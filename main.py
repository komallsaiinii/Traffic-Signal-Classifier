import numpy as np
import matplotlib.pyplot as plt

from PIL import Image
import os
import pandas as pd

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

data = []
labels = []
classes = 43
cur_path = os.getcwd()
data_folder = 'data'

# Loading training dataset
for i in range(classes):
    print(i)
    path = os.path.join(cur_path, data_folder, 'train', str(i))
    images = os.listdir(path)

    for a in images:
        print("Image loading")
        try:
            image = Image.open(path + '\\' + a)
            image = image.resize((30, 30))
            image = np.array(image)
            data.append(image)
            labels.append(i)
        except:
            print("Error loading image")

# # Converting lists into numpy arrays
# data = np.array(data)
# labels = np.array(labels)
#
# print("Lists to Numpy completed")
#
# # Normalizing data via Min-Max normalizer
# scaler = MinMaxScaler()
# ascolumns = data.reshape(-1, 3)
# t = scaler.fit_transform(ascolumns)
# data = t.reshape(data.shape)
# print(data.shape, labels.shape)
#
# print("Data Normalized")
#
# # Splitting training and validation dataset
# X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)
#
# print("Data Splitted")
#
# print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)
#
# # Converting the labels into one hot encoding
# y_train = to_categorical(y_train, 43)
# y_val = to_categorical(y_val, 43)
#
# print("Labels into hot encoding converted")
#
# # Building the model
# model = Sequential()
# model.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu', input_shape=X_train.shape[1:]))
# model.add(MaxPool2D(pool_size=(2, 2)))
# model.add(Dropout(rate=0.25))
# model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))
# model.add(MaxPool2D(pool_size=(2, 2)))
# model.add(Dropout(rate=0.25))
# model.add(Flatten())
# model.add(Dense(256, activation='relu'))
# model.add(Dropout(rate=0.5))
# model.add(Dense(43, activation='softmax'))
# model.summary()
# print("model built")
#
# # Compilation of the model
# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
# print("Model Complied")
#
# epochs = 15
# history = model.fit(X_train, y_train, batch_size=32, epochs=epochs, validation_data=(X_val, y_val))
# model.save('traffic_classifier.h5')
#
# # Plotting graphs for accuracy
# plt.figure(0)
# plt.plot(history.history['accuracy'], label='training accuracy')
# plt.plot(history.history['val_accuracy'], label='val accuracy')
# plt.title('Accuracy')
# plt.xlabel('epochs')
# plt.ylabel('accuracy')
# plt.legend()
# plt.show()
#
# plt.figure(1)
# plt.plot(history.history['loss'], label='training loss')
# plt.plot(history.history['val_loss'], label='val loss')
# plt.title('Loss')
# plt.xlabel('epochs')
# plt.ylabel('loss')
# plt.legend()
# plt.show()
#
# # Testing the model
# path = os.path.join(cur_path, data_folder)
# y_test = pd.read_csv(data_folder + '/' + 'Test.csv')
# labels = y_test["ClassId"].values
# imgs = y_test["Path"].values
#
# data = []
#
# for img in imgs:
#     image = Image.open(path + '\\' + img)
#     image = image.resize((30, 30))
#     data.append(np.array(image))
#
# X_test = np.array(data)
#
# # Normalizing test set
# ascolumns = X_test.reshape(-1, 3)
# t = scaler.transform(ascolumns)
# X_test = t.reshape(X_test.shape)
#
# # Predicting on test set
# pred = np.argmax(model.predict(X_test), axis=1)
#
# # Performance evaluation
# cm = confusion_matrix(labels, pred)
# print('Confusion Matrix:')
# print(cm)
#
# # accuracy: (tp + tn) / (p + n)
# accuracy = accuracy_score(labels, pred)
# print('Accuracy: %f' % accuracy)
# # precision tp / (tp + fp)
# precision = precision_score(labels, pred, average='macro')
# print('Precision: %f' % precision)
# # recall: tp / (tp + fn)
# recall = recall_score(labels, pred, average='macro')
# print('Recall: %f' % recall)
# # f1: 2 tp / (2 tp + fp + fn)
# f1 = f1_score(labels, pred, average='macro')
# print('F1 score: %f' % f1)
